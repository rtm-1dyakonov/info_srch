# Инфопоиск (1, 2, 4, 5)
## Раздьяконов Артём, группа S41012. 
Выполненные задания: 1, 2, 4, 5

1)  Для подготовки данных я использовал скрапер wiki-страниц, который случайным образом подобрал страницы по 8-ми непересекающимся темам, случайно выбирая взаимо-релевантные страницы внутри каждой темы:
  * игроки и тренеры лиги NHL
  * метрополитен в городах Азии
  * породы спортивных скаковых лошадей
  * индонезийские телесериалы
  * игроки лиги CFL
  * игроки лиги MLB
  * оперные исполнители и оперные песни (преимущественно итальянские)
  * провинции Италии
Выборка в общем плане включает ~160 страниц
2) Строчки нормализуются с помощью стеммера SnowBall и фильтрации английских стопслов. Для использования подхода tf-idf был применён TfidfVectorizer из sklearn.feature_extraction.text. Модель сохраняется в папке vectorization, файл vectorizer.pk.
3) Оценка качества производится по метрикам precision, recall, f1 и mrr.
4) Для обработки изображений, которые сохранятся с названием, соответствющим классу объекта, задетектированному на фотографии, после выполнения кода в папке images внутри директории cv_part, используется модель Xception с приведением изображений к размерам 299 на 299.
5) Так как в файле разметки к каждой странице предлагается несколько запросов и они намеренно нетривиальные, то метрика mrr варьируется между значениями 0.59 - 0.7.
6) Файл разметки подготовлен по той идее, что доки/страниц html могут быть в нескольких кластерах одновременно, то есть можно сказать, что один запрос - один кластер.
7) Подготовка файла разметки произовдилась с помощью блокнота *markup_prep.ipynb*


## Запуск проекта (через интерфейс командной строки, в процессе разработки использовался VS Code)
1. Установка всего необходимого (nltk, bs4, keras, translate)

```pip install -e .```

2. Иногда с пакетом stopwords возникают проблемы, так что на всякий случай лучше прописать следующее

```python -m nltk.downloader stopwords```

3. Создать пустую папку images внутри директории cv_part

4. Создание модели на основе алгоритма tf-idf по собраным html-странциам, который лежат в папке html

```tfidf "<абсолютный/путь/до/папки/html/>```

5. Для того, чтобы получить 2 самых близких/релевантных по косинусному расстоянию страницы по введённому запросу, нужно ввести следующее

```request "<запрос>"```
Чтобы изменить количество релевантных страниц, нужно поменять константу _max_results_ в скрипте run.py

6. Для вычисления метрик, которые буду подсчитаны по всем запросам и выведены строчкой в консоли, по запросам из файла разметки нужно прописать

```quality "<абсолютный/путь/до/файла/markup.csv>"```
